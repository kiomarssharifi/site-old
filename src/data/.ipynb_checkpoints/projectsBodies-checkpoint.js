import "./projectBodies.css";
import Zoom from 'react-medium-image-zoom'
import 'react-medium-image-zoom/dist/styles.css'

const ProjectBody01 = () => {
    return (
        <div>
            <em style={{fontSize: "15px"}}>Last updated: 17 Nov, 2023</em>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="SpartanX Logo"
                            src="../projects_spartan_x_01.jpg"
                            width="200"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        SpartanX
                    </em></figcaption>
                </figure>
            </div>

            “Is it possible?” This was the first question every person asked me when I explained what we do in our
            startup. At times, I felt hopeless about the results of our work. However, the primary reason we persevered
            in our struggle was the realization that major companies, such as <a target="_blank" rel="noreferrer"
                                                                                 href="https://www.rentec.com/Home.action?index=true">RenTech</a> hedge
            fund and <a target="_blank" rel="noreferrer"
                        href="https://www.blackrock.com/corporate">BlackRock</a> investment management, have
            successfully employed similar approaches, serving as examples of success.
            <br/><br/>
            In Cardano Trader, our goal was to create a fully automated system for managing assets in the Tehran Market
            using AI, particularly ML and DL methods. I dedicated two and a half years to this endeavor, collaborating
            with two friends who joined me later. We worked day and night on this project. Now, with two years having
            passed since the failure of our startup and the expiration of our NDA, I aim to share some insights about
            our journey, highlighting both the challenges and successes we experienced in implementing this idea in the
            real world.
            <br/><br/>
            The system comprises three main components: data listener, an AI core, and an accounts manager.
            The data listener component functioned as a scraper, collecting technical data primarily from two
            sources—the market's official page and the broker's page. This part's responsibility was to publish data in
            memory, and to minimize delays, the data was directly written to <a target="_blank" rel="noreferrer"
                                                                                href="https://www.softprayog.in/programming/interprocess-communication-using-system-v-shared-memory-in-linux#:~:text=Shared%20memory%20is%20one%20of,the%20message%20queues%20and%20semaphores.">shared
            memory</a>. The
            AI core served as the hub for various algorithms employing different strategies. These
            algorithms determined, at each time frame, the selection of symbols (where we could invest our money in
            different symbols in the market) and the portion of the entire asset to be allocated. We implemented three
            various algorithms for this segment, with SpartanX, representing a fine-tuned version of
            Spartan, being one of them. Lastly, the accounts manager, also a scraper, managed orders generated by each
            algorithm in the AI core,
            facilitating their submission, modification, or cancellation on the exchange page for multiple accounts.
            <br/><br/>
            In this article, I aim to elucidate SpartanX and the concept behind it. I won’t delve into the other
            components and algorithms, as they warrant separate articles. Initially, I will delve into the origin of
            the SpartanX idea by presenting results from its precursor concepts. Then, I will provide a detailed
            explanation. Finally, I will present real-world test results and conclude the article by addressing
            practical issues.
            <br/><br/>
            <div className="heading-1">Before Going Deeper</div>
            <br/><br/>
            It is worthwhile to mention that in real-world applications, there are many things you need to do beyond
            just loading data and training a model. Consider the following aspects to be addressed in a full data
            science project: gathering and cleaning data, reconstructing missing data, hardware requirements,
            versioning and checkpointing the models, visualization, and validation, real-time monitoring, testing, and
            retraining based on test results.
            <br/><br/>
            <div className="heading-1">About Tehran Market</div>
            <br/><br/>
            The Tehran Stock Exchange is Iran’s largest stock market, with an estimated daily transaction value of about
            $120 million. At the time we developed our system, there were approximately 750 active companies, and their
            shares could be traded as symbols.
            <br/><br/>
            The market has some restrictions and rules that pose challenges for a data scientist. For example, the
            market opens at 9 AM and closes at 12:30 PM, although the opening hours have occasionally changed.
            Additionally, each day, each symbol must fluctuate within a pre-determined range based on yesterday's
            closing price. This range varies for different symbols. Moreover, trading for each symbol must halt due to
            regulatory requirements, leading to the blocking of all assets. Designing a system must take these scenarios
            into consideration.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="SpartanX Logo"
                            src="../projects_spartan_x_02.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        The price of the symbol DELOR in Rials (the currency of Iran) is depicted from mid-April to
                        mid-October 2020. The price is represented by dots for different days, with colors indicating
                        positive/negative trends—red, white, or green. White rectangles indicate the price limits for
                        each day. The red and grey vertical areas represent days when the symbol's trade is restricted.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Start: Prediction of the Future of Each Symbol</div>
            <br/><br/>
            The initial idea involves predicting the future price of each symbol and subsequently selecting the best
            symbols based on both their anticipated future and the confidence level of these predictions. For instance,
            if we estimate that symbol X's price will increase by 5% in 7 days with a confidence level of 70%, and
            symbol Y's price will rise by 3.5% in 7 days with a confidence level of 80%, we can allocate our assets
            among these two symbols according to our risk threshold. Another consideration is reserving a portion of our
            assets for potential future opportunities, a decision that is intricate and will be discussed later.
            <br/><br/>
            At the start of our startup, we trained various regression models and networks to predict the future price
            of each symbol. However, we encountered several challenges:
            <br/>
            <ul>
                <li>
                    Determining the prediction horizon: How many future points should be considered—next minute, next
                    hour, next day, next week, or next month?
                </li>
                <li>
                    Measuring prediction confidence: How can we quantify the confidence of our predictions?
                </li>
                <li>
                    Feature selection: What data should be included as features? Should additional economic measures
                    like MACD, EMA, and RSI be used?
                </li>
                <li>
                    Model training: Should a single model be trained for all symbols, or is it more effective to use
                    separate models for different symbols?
                </li>
                <li>
                    Non-stationary price history: The price history is non-stationary and varies over time and among
                    symbols. How can we address this variability?
                </li>
            </ul>
            <br/>
            Initially, we aggregated all symbols throughout their entire history to create a comprehensive dataset.
            Various features, such as MACD, were computed for each price and incorporated into our dataset. To address
            the non-stationary problem, we implemented a moving window of a specified size and utilized the min-max
            normalization method to maintain the price range within a specific interval. As for the model's response, we
            calculated the relative price change, representing the change in future price relative to the current price.
            <br/><br/>
            Despite the involvement of all symbols and a dataset size of approximately 200,000 samples, the abundance of
            features posed the curse of dimensionality problem for the model. To address this, we initially employed a
            Random Forest model to identify the most informative features, ultimately narrowing them down to about 20
            key features.
            <br/><br/>
            Over more than two months, we trained various models to predict future prices, but the results were
            consistently discouraging: almost always yielding predictions of zero. Regardless of changing both the model
            architecture and the dataset, the predictions remained stuck at zero. We explored a range of models,
            including RNN, LSTM, GRU, 1-dimensional CNN, Gaussian Process regression, and even Transformer.
            Additionally, we experimented with different selections of economic measures as features, various
            normalization methods, diverse time horizons from hours to weeks, and varied selections of data related to
            different dates in the market.
            <br/><br/>
            <div className="heading-1">What is the Meaning of Zero?</div>
            <br/><br/>
            We arrived at two possible explanations for our results. First, it might be a flawed assumption to aggregate
            all market data together. It is plausible that symbols' prices vary in distinct ways. For instance, Symbol
            X's price could rise under one specific condition, while under the same condition, the price of Y might
            drop. Moreover, this rule for a single symbol can change over time. The same condition that contributed to
            the rise in the price of Symbol Z two years ago may not have the same effect this year due to the overall
            market being in a harsh condition.
            <br/><br/>
            Secondly, we considered that the cause of price change within the time frame we examined might not be
            adequately represented in our features. For instance, if we aim to predict the price of Symbol Z in the next
            month, relying on daily trade value, current price, RSI, and similar features might be insufficient.
            Instead, the most significant determinant could be found in the net sales of the company. Unfortunately,
            thess types of fundamental data are not present in our dataset, and we do not have access to them.
            <br/><br/>
            <div className="heading-1">SpartanX</div>
            <br/><br/>
            So, we decided to shift our perspective and implemented three major changes. The first change involved
            time-dependent clustering of symbols. Our assumption was that the price dynamics of each symbol vary
            smoothly over time (with time referring to the day in this context). On any given day, all 750 symbols are
            grouped into 10 different clusters based on certain features extracted from their price fluctuations.
            <br/><br/>
            The second change involved approaching the problem as a classification problem rather than a regression one.
            In fact, accurately predicting the exact future price is challenging. When consulting an expert about
            a symbol, they often provide general directional insights, such as "It will go up" or "It will drop." In
            practice, the focus is more on understanding the overall trend rather than pinpointing the exact price.
            <br/><br/>
            Indeed, we labeled our data at each point as either a positive or negative sample. The label signifies that
            buying
            the share at that time point could result in a benefit. However, a challenge arises: At which point sell
            that share? How much benefit do you want to make and sell your share? Do you want to hold the share if the
            price drops, perhaps for a return? The classification of good/bad labels is highly dependent on the chosen
            trading strategy.
            <br/><br/>
            The third change involved reducing the prediction horizon to one hour. After thorough examination, I can
            confidently state that there is no substantial information in the technical data of the market for
            predicting the long-term future of a symbol, such as a month later. The market, being influenced by
            significant players, involves human decision-making based on several unpredictable factors. However, in a
            short time period, we observed that the price exhibits a discernible dynamic.
            <br/><br/>
            This entails a trade-off between profit and prediction ability. Lowering the future horizon allows for
            better price prediction, but short-term profits are typically modest. This is where the concept of
            High-Frequency Trading comes into play. In this method, numerous small trades are executed, and the
            cumulative result of these trades amounts to a significant profit.
            <br/><br/>
            Hence, SpartanX is divided into two interconnected sub-systems: the Signal Generator and the Stream Manager.
            The Signal Generator generates, every 2.5 seconds, a vector of probabilities for all symbols. Each entry in
            the vector represents the goodness of buying that symbol at that specific time (not predicting the future
            price
            value). The Stream Manager is responsible for managing the entire asset and utilizing it to invest in
            various symbols.
            <br/><br/>
            <div className="heading-1">Signal Generator</div>
            <br/><br/>
            At the beginning of each day, the Signal Generator extracts 5 features representing the price fluctuations
            of each symbol over the last 30 days. This results in a 5x30 matrix for each symbol. Then, we
            utilized <a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/1802.03426">UMAP</a> to
            reduce these 150 features to 2 dimensions. Subsequently, we used <a target="_blank" rel="noreferrer"
                                                                                href="https://link.springer.com/chapter/10.1007/978-3-642-37456-2_14">HDBScan</a> to
            categorize all 750 symbols into 10 clusters based on their reduced 2 features. This process is repeated
            daily, assigning each symbol to one of the 10 clusters. It is worth noting that a symbol may be in a
            different cluster today than it was 20 days ago. To account for variations in UMAP reduction and HDBScan
            clustering, we employed <a target="_blank" rel="noreferrer"
                                       href="https://en.wikipedia.org/wiki/Procrustes_transformation">Procrustes</a> to
            align these features for consecutive days.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Symbols Clustered"
                            src="../projects_spartan_x_03.gif"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Symbols are clustered, with each dot representing the reduced features of each symbol in 2
                        dimensions. Distinct colors correspond to each of the 10 clusters. The transition of clusters is
                        depicted in each frame of the animation, with dates annotated beneath the figure.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            We found the 10 clusters to be intriguing as they exhibited a high sensitivity to related symbols. For
            instance, the symbol Khodro consistently appeared as the closest symbol to Vsapa on most days. This
            alignment is meaningful as both companies are prominent automobile manufacturers and share significant
            similarities. It is noteworthy that we didn't explicitly feed this knowledge to our model; rather, it
            emerged
            based on price variability, effectively identifying and recovering related symbols.
            <br/><br/>
            Examining the price of sample symbols across various clusters revealed a compelling pattern. In essence, our
            approach enabled us to segregate symbols based on their liquidity. Less liquid symbols exhibited more
            significant price jumps, while more liquid symbols proved resistant to large price changes due to the
            substantial support from high volumes of buy and sell orders. This separation not only provided valuable
            insights into market dynamics but also paved the way for tailored trading strategies, recognizing that
            trading in different symbols necessitates distinct approaches contingent on the liquidity of each symbol.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Sample Price Clustered"
                            src="../projects_spartan_x_04.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Sample price changes for each cluster in a single day are illustrated, with the y-axis
                        representing the percentage change in price.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            Then, we trained different models for these clusters, recognizing that the price dynamics differ
            among them. The model's output, as discussed earlier, signifies the desirability of buying. A higher
            probability suggests a potentially larger profit when buying shares of that symbol at that time. For
            labeling data, we established a simple trading strategy: we buy a share at time 't' with price 'p.' If any
            of the following criteria are met, we sell that share:
            <br/>
            <ol>
                <li>
                    If the current price falls by less than 2% of the maximum price in the time window from time 't' to
                    the present.
                </li>
                <li>
                    If the price falls by less than 1% of 'p.'
                </li>
                <li>
                    If the current time surpasses 1 hour from time 't.'
                </li>
            </ol>

            <br/>
            We iterated through all time points (sampled with a 2.5-second period) in the dataset for each day and apply
            these criteria. If the sell price is higher than the buy price plus the exchange fee, that time point is
            labeled as 'good.' Conversely, the remaining points are labeled as 'bad.'
            <br/><br/>
            By training our model in this manner, we ensured that if the model accuracy is high, and we follow these
            predictions along with the aforementioned trading strategy, we can benefit from the market.
            <br/><br/>
            We utilized a <a target="_blank" rel="noreferrer"
                             href="https://arxiv.org/abs/1706.03762">Transformer</a> network
            for this purpose due to its foundation on the attention mechanism. Unlike
            RNN, which struggles with inferring patterns occurring at different time scales, the Transformer, leveraging
            Multi-Head Attention blocks, effectively addresses this limitation. To tailor the Transformer for our
            problem, we made specific adjustments to the architecture, such as removing embedding and modifying
            positional encoding.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Transformer Response"
                            src="../projects_spartan_x_05.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Sample predictions of the Transformer are depicted. The start of each red line signifies points
                        where the model predicted them as good places to invest (buy), while the end of the red line
                        indicates sell points according to the trade strategy.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Stream Manager</div>
            <br/><br/>
            In an imaginary scenario where we have access to a perfect model that predicts the future of each symbol
            with high confidence, say 90%, determining the best approach for investment becomes nuanced. While one might
            suggest investing all assets in the share of a symbol the first time, there is a 10% chance it could be a
            bad
            decision leading to losses. Alternatively, waiting for a better opportunity to buy another symbol and
            potentially gain greater benefits is also a consideration. Adding to the complexity, if we already hold
            shares of symbol X and it is currently in a loss, the decision of whether to sell those shares to free up
            assets for a symbol with a higher probability introduces another layer of complexity to the investment
            strategy.
            <br/><br/>
            In my opinion and based on my experience, the selection of these strategies holds more significance than
            predicting the future price. This aspect is closely tied to economic topics such as risk management and
            asset management.
            <br/><br/>
            We devised a simple yet effective strategy for this. Initially, our entire asset, let's say 1, is divided
            into a specified number of parts, denoted as 'streams,' let's say N. In each trade, we utilize one of these
            streams to buy and sell shares. At each time point, the system examines all available streams. If one of
            them is available, the Stream Manager consults the Signal Generator for promising symbols to buy. If the
            probability of a symbol exceeds a threshold and that symbol has not been purchased before, the stream is
            allocated to that symbol. The Stream Manager continuously monitors all active streams, and, as previously
            outlined, if any of these criteria are met, the shares of that symbol are sold:
            <br/>
            <ol>
                <li>
                    If the current price falls by less than 2% of the maximum price in the time window from time 't' to
                    the present.
                </li>
                <li>
                    If the price falls by less than 1% of 'p.'
                </li>
                <li>
                    If the current time surpasses 1 hour from time 't.'
                </li>
            </ol>

            <br/>
            The parameters of this method play a crucial role in determining the overall benefit. For example, if the
            number N is increased, the profit will decrease since the total profit is divided by N; simultaneously, the
            risk will decline. Therefore, finding the optimal set of parameters is highly impactful and poses a
            challenging problem. We identified the best set through grid searching across all possible combinations over
            a 3-week period, using five powerful machines with a Core i9 CPU. The following image illustrates the
            impact of changing some of these parameters on the overall profit.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Trading Parameters"
                            src="../projects_spartan_x_06.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        The impact of varying distinct parameters of the trade strategy on daily profit is illustrated.
                        It is important to note that the profit can also be negative, emphasizing the potential
                        results of an incorrect choice of parameters, despite the Signal Generator perform well.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            Recall the labeling mechanism in the Signal Generator part. It is important to note that labels are
            dependent
            on this parameter set. The best parameter set and the profit are also influenced by the output of the Signal
            Generator. Yet, we should fine-tune these two sub-systems together. Fine-tuning these two sub-systems
            together is a non-trivial task, and currently, we fine-tune Transformer models after identifying the best
            set.
            <br/><br/>
            <div className="heading-1">Results and Discussion</div>
            <br/><br/>
            The entire structure underwent multiple tests. In a test spanning approximately one and a half months with
            an initial asset of $100,000, we achieved a 15% gain. Notably, in one day, two streams were bought and sold
            in a symbol within just 6 seconds. However, upon further analysis, we discovered that this profit is heavily
            influenced by market circumstances. High-frequency trading necessitates high liquidity as orders must be
            executed swiftly. Additionally, this strategy performs well in neutral markets, exhibiting effectiveness
            neither in a bullish nor a bearish trend.
            <br/><br/>
            This strategy encountered two practical issues during real tests, resulting in a decline in overall
            performance. Firstly, buy orders often failed to complete, primarily due to liquidity issues. Secondly, as
            mentioned earlier, the Tehran exchange market's rule stipulates a specific price range. Consequently, during
            a downward trend, shareholders seek to sell their shares, leading to a lack of buyers and causing a price
            saturation with a lower bound. This, in turn, results in some streams being blocked for several days,
            preventing their use in other symbols for profit generation. These challenges significantly impacted the
            algorithm's performance.
            <br/><br/>
            As suggested earlier, the most crucial factor in this type of work is not finding the best machine to
            predict the future price; rather, it is more important to implement strategies and methods for
            decision-making based on the current situation. Reinforcement learning may offer a promising avenue. By
            defining agents capable of taking actions like creating and canceling orders, and modeling rewards as the
            profit from trading within a specific time period based on probabilities generated by the Signal Generator,
            a more adaptive approach could be achieved. Although we attempted to pursue this direction, time constraints
            hindered us from completing the task, ultimately leading to the collapse of our startup.
        </div>
    );
};


const ProjectBody02 = () => {
    return (
        <div>
            <em style={{fontSize: "15px"}}>Last updated: 19 Nov, 2023</em>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img className="image-1"
                            alt="Image2Latex Logo"
                            src="../projects_image_to_latex_01.jpg"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Conversion of math image to LaTex code
                    </em></figcaption>
                </figure>
            </div>

            One of the most challenging aspects of writing an academic paper with LaTeX is inserting mathematical
            equations and formulas. One approach is to write the formula on a piece of paper, take a picture of it, and
            then use Optical Character Recognition (OCR) engines to convert it into a typed equation. While there are
            already OCR engines that can convert handwritten text into typed text, there are a few examples that
            specialize in handling mathematical images.
            <br/><br/>
            In this article, I am going to explain how I utilized deep neural networks to convert a computer-generated
            image of a mathematical formula into LaTeX code. It is worth noting that handling handwritten images
            presents more challenges. I employed innovative methods for this task as part of my deep learning course
            project.
            <br/><br/>
            The main concept is attention. When writing a formula, the process involves reading from left to right,
            writing each token while maintaining focus on that token and its position within the entire formula. This
            approach is akin to the process of generating a caption for a figure. In a <a target="_blank"
                                                                                          rel="noreferrer"
                                                                                          href="https://arxiv.org/abs/1502.03044">specific
            method</a>, a
            CNN network extracts the feature map of an image, followed by the use
            of a language model with an attention mechanism. This model sequentially generates each token while
            selectively focusing on different locations within the feature map of the image.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Show, Attend and Tell"
                            src="../projects_image_to_latex_02.png"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Show, Attend and Tell from <a target="_blank" rel="noreferrer"
                                                      href="https://zhuanlan.zhihu.com/p/32333802">this link</a>
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            In this approach, the provided image undergoes initial processing through a CNN network, extracting the
            image's feature map. This map serves as the foundation for the initial state of the RNN. At each state, the
            RNN generates the predicted output vocabulary and produces a map of the same size as the feature map. This
            map functions as the attention mechanism. The map is multiplied by the extracted feature map, and the result
            becomes the input for the subsequent RNN step. Consequently, in generating each vocabulary element, both the
            previous vocabulary and the attention result are fed into the RNN. The expectation is that, for each
            vocabulary element, the feature map adjusts the pixels corresponding to the relevant part of the image,
            effectively filtering out irrelevant areas.
            <br/><br/>
            While the idea is promising, there is speculation regarding the actual functioning of attention as
            anticipated. The absence of a guarantee that the attention map consistently selects the relevant part of the
            image for generating each output vocabulary raises concerns. Additionally, the entire structure is trained
            solely based on the loss computed at the output, without assurance that every aspect of the structure is
            effectively trained, especially considering the issue of gradient vanishing in RNNs. In response to these
            concerns, I have modified the aforementioned structure with the following suggestions:
            <br/>
            <ul>
                <li>
                    Utilize Inception modules as CNN blocks due to their capability to handle different sizes of various
                    tokens in the input image.
                </li>
                <li>
                    Pre-train the Inception modules in another task with specific labeling. Enforce the Inception
                    network to initially identify the bounding box of the entire given image, ensuring that the
                    attention mechanism aligns with expectations.
                </li>
                <li>
                    Apply a Transformer network with skip-gram embedding and the bounding box output of the Inception
                    network to translate math tokens (markups) into math characters using an attention mechanism.
                </li>
            </ul>
            <br/>
            <div className="heading-1">Data Exploration</div>
            <br/><br/>
            The dataset, consisting of nearly 140k samples of math images paired with LaTeX code, can be accessed
            through <a target="_blank" rel="noreferrer" href="https://untrix.github.io/i2l/140k_download.html">this
            link</a>.
            The LaTeX code corpus comprises around 550 unique vocabularies,
            such as &#123;, Q, ^, \mu, and \frac. While the vocabulary size may not be extensive,
            the challenge is more complex than initially anticipated. Some vocabularies are combined to form
            intricate markups, such as \begin&#123;array&#125;, \begin&#123;matrix&#125;,
            and \begin&#123;tabular&#125;.
            Additionally, certain vocabularies lack a corresponding markup as output, including \ref and \label.
            Some are positioned above, below, or in proximity to the next vocabulary,
            such as \sqrt, \widehat, \widetild, \matrix, \over, \fbox, \underline,
            and \choose.
            Lastly, there are symbols that influence the appearance of the succeeding vocabulary output, such
            as \mathrm, \mathsf, \mathcal, \mbox, \rm, \scriptscriptstyle, \sf, \textbf,
            and \textit.
            <br/><br/>
            <div className="heading-1">Unique Markups</div>
            <br/><br/>
            By examining each combination of distinct markups, particularly those mentioned in the previous section, I
            identified 2608 unique visual markups. To achieve this, I initially extracted all 550 unique vocabularies
            from the training labels. Subsequently, I checked each vocabulary in various examples to pinpoint those
            causing the above difficulties. Finally, I created a simple .tex file template and used a for loop to
            automatically compile the .tex document, saving the output images in a directory. The following image
            displays some samples.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Example unique markups"
                            src="../projects_image_to_latex_03.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Example unique markups
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Finding Bounding Boxes</div>
            <br/><br/>
            After identifying all unique markups, I proceeded to determine the location of each markup and annotated it
            with a rectangular bounding box. This step is crucial for defining attention blocks. Indeed, what I did here
            is extracting a guide for the attention mechanism. I will delve into this in more detail later; for now,
            let's focus on the task of locating bounding boxes for all markups.
            <br/><br/>
            Traditional image processing methods, particularly the template matching approach, prove to be a valuable
            tool for this purpose. I utilized a well-known technique called template matching, which involves locating a
            specific pattern within a given image. In our case, the objective is to identify unique markups in the input
            image. The process entails sliding the unique markup across the image, calculating the correlation between
            the markup and each location it passes through, resulting in a correlation matrix. The peak index in this
            matrix corresponds to the location where the markup most closely resembles the pattern at that point. To
            enhance this process, I employed a more sophisticated method known as <a target="_blank" rel="noreferrer"
                                                                                     href="https://docs.opencv.org/4.x/df/dfb/group__imgproc__object.html">TM_SQDIFF_NORMED</a>,
            which normalizes these correlations.
            <br/><br/>
            However, there are two challenges with this method. First, the size of the specific markup in the image may
            deviate from the original markup. Second, executing this method for all 2608 unique markups proves to be
            highly time-consuming. Fortunately, solutions exist to address these issues.
            <br/><br/>
            To address the issue of varying sizes, we can employ template matching with different sizes of the markup.
            Estimating the range of sizes involves comparing the size of the image with that of the markup.
            For the second problem, using the label of the image in the dataset (the LaTeX code) proves to be a useful
            strategy. Extracting the used markups and their frequency of occurrence can be easily achieved by reading
            the LaTeX code. It is important to be cautious with specific vocabularies such as \mathcal and \textbf that
            generate combined markups.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="bounding boxes"
                            src="../projects_image_to_latex_04.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Sample identified bounding boxes in the images
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Assigning a Vector for Each Markup</div>
            <br/><br/>
            The objective in this section is to assign a compact vector to each of the 2608 unique markups. These
            vectors will be utilized in the subsequent pre-training of the CNN network.
            <br/><br/>
            The basic idea involves using dimensionality reduction tools, such as PCA, to condense the pixel space to a
            few numbers. However, it is crucial to note that the distribution of pixels within the markups does not
            adhere to a normal distribution. In reality, markups are grouped together, with some highly correlated
            within their respective group but distant from other correlated groups. Consequently, PCA is not the most
            suitable approach for this purpose.
            <br/><br/>
            For this purpose, I employed a novel approach. Initially, I utilized KMeans to segregate highly correlated
            images into different clusters. Subsequently, I applied PCA independently in each cluster. The final vector
            for each markup is generated by concatenating two vectors. The first component is the center of the
            corresponding cluster, reduced to 5 dimensions by applying PCA to all central points. The second component
            is the 11-dimensional vector obtained by applying PCA to the markup within its cluster. Consequently, each
            16-dimensional vector for a markup encapsulates two key aspects: 1) the cluster to which the markup belongs,
            and 2) the precise location of the markup within that cluster.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="pca markup"
                            src="../projects_image_to_latex_05.png"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Each markup is represented by a dot in a two-dimensional space, with each dot corresponding to
                        the assigned vector for the respective markup.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Training CNN Network</div>
            <br/><br/>
            As a recap, our goal is to implement an attention mechanism to address two key questions: 1. What markups
            constitute the given image? 2. Where is the location of each markup within the image? With the completion of
            the preceding steps, we have already addressed these questions. The 16-dimensional vector assigned to each
            markup represents the markup itself, while the bounding box identified for each markup indicates its
            location. With this information, we are now ready to proceed with the pre-training of our CNN network.
            <br/><br/>
            I utilized inception modules at different layers of the network because this architecture is well-suited for
            handling different sizes of input images. Given that distinct markups are presented in the input image with
            varying sizes, the network needs to be capable of effectively capturing these differences.
            <br/><br/>
            I created mock labels for training this network. Assuming the input image size is 60x360, I generated a
            label map with dimensions 60x360x16. Let's take the example of the markup 'A' appearing in the input image
            with a bounding box from point [30, 110] to point [40, 120]. The label map in this specified rectangle is
            filled with the vector assigned to markup 'A.' This process is iteratively applied to all markups in the
            image. Then, the network is trained to predict this label map.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="sample BB"
                            src="../projects_image_to_latex_06.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        The sample image is displayed at the top, accompanied by its mock label map on the left and the
                        CNN network's prediction on the right. Each row in the label map and prediction corresponds to a
                        specific third dimension (16 layers overall).
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            By employing this approach, we ensure that the CNN network is pre-trained to attend to various locations
            within the image. The network is now prepared to be integrated into the attention mechanism to extract the
            LaTeX code.
            <br/><br/>
            <div className="heading-1">Transformer</div>
            <br/><br/>
            In the final step, a language model is employed to translate these markups considering their respective
            locations in the image. The Transformer architecture, known for its reliance on the attention mechanism, is
            chosen for this task. The extracted feature maps from the CNN inception network serve as the input for the
            Transformer, which sequentially generates each vocabulary at every time step.
            <br/><br/>
            In the embedding section of the Transformer, I employed the skip-gram method to pre-train the embedding
            matrix. Additionally, certain parts of the network, notably the positional encoding block, were modified to
            accommodate three-dimensional images. The provided image illustrates a sample input image along with its
            predicted LaTeX code.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="transformer results"
                            src="../projects_image_to_latex_07.png"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Results of Transformer network with attention to different locations for generating markups.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Conclusion</div>
            <br/><br/>
            This article addresses the difficulty of integrating mathematical equations into LaTeX papers and proposes a
            solution using deep NNs with attention mechanisms. I modified the structure to enhance attention and
            suggested using Inception modules in CNN blocks. I presented challenges in handling unique vocabularies. The
            process involves finding bounding boxes, assigning vectors to markups, and training a CNN network. The final
            step employs a Transformer architecture to translate markups into LaTeX code based on their locations in the
            image.
            <br/><br/>
            In my opinion, all steps were executed successfully except for the final one. The Transformer network did
            not generate the codes as anticipated. For future work, readers are encouraged to propose improved
            structures to enhance the performance of the Transformer network.
        </div>
    );
};


const ProjectBody03 = () => {
    return (
        <div>
            <em style={{fontSize: "15px"}}>Last updated: 20 Nov, 2023</em>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img className="image-2"
                            alt="EMG Sensors Logo"
                            src="../projects_emg_sensors_01.jpeg"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        EMG sensors
                    </em></figcaption>
                </figure>
            </div>

            Imagine you want to create an artificial arm attached to your own arm to assist in carrying heavy objects.
            This arm could also be beneficial for individuals with damaged arms as part of their rehabilitation.
            Developing such a robot involves an interdisciplinary process that combines electrical, mechanical, and
            chemical engineering. The design of the joints and the materials chosen for this purpose, along with the
            associated hardware and software requirements, play a crucial role in achieving success in this endeavor.
            <br/><br/>
            In this article, I will show the steps involved in designing and implementing electrical components, along
            with the utilization of data acquisition and data processing tools. The collection and processing of data
            through a structured infrastructure constitute basic stages in the development of a complete artificial arm.
            <br/><br/>
            Several sensors are employed for this purpose, including an accelerometer for position estimation, a
            gyroscope for angle estimation, a camera for capturing various parts of the arm, and EMG sensors for
            measuring muscle activity—these are among the most critical sensors utilized. A complete application
            leverages all of these sensors, aggregating their data to estimate all state variables of the arm and reduce
            the impact of noise.
            <br/><br/>
            I employed six EMG sensors in this project to estimate the forces applied by muscles in four distinct tasks:
            1) wrist flexion (for grasping objects in the hand), 2) forearm supination, 3) forearm pronation (for
            rotating objects held in the hand), and 4) elbow flexion (for applying additional force when lifting heavy
            objects). I recorded signals from six muscles involved in these tasks: Biceps Brachii, Triceps Brachii,
            Flexor Carpi Radialis, Extensor Digitorum, Flexor Carpi Ulnaris, and Abductor Pollicis Brevis.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="task description and hand mucsles"
                            src="../projects_emg_sensors_02.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Three tasks (left) adapted from <a target="_blank" rel="noreferrer"
                                                           href="https://tommorrison.uk/">this link</a> and
                        recorded muscles (right) adapted from <a target="_blank" rel="noreferrer"
                                                                 href="https://nursing.unboundmedicine.com/nursingcentral/view/Tabers-Dictionary/742111/all/arm">this
                        link</a>.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            The final goal of this project is to establish a real-time system for classifying the current status of the
            arm and hand. In the following sections, I will begin by describing the hardware and software platforms.
            Next, I will elaborate on the concept of low-delay filtering for these signals. Finally, the implementation
            of a random forest for classification and its corresponding results will be presented.
            <br/><br/>
            <div className="heading-1">Hardware and Software Design</div>
            <br/><br/>
            To understand the recording of muscular activity, it is essential to understand how muscles are activated
            through motor neurons. These neurons generate electrical impulses, known as action potentials, within muscle
            fibers. These impulses traverse through layers of fat and skin, eventually reaching the skin's surface.
            Placing two electrodes on different areas of the skin allows us to capture the difference between them,
            representing a function, such as a simple gain, of these impulses. It is important to note that these
            impulses have a small amplitude, typically around 5mV. Additionally, the electrodes pick up distinct
            impulses from various cells, resulting in the observed signal being a summation with different gains and
            polarities. When we amplify and record from the skin, the signal we observe reflects this complex summation.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="raw data"
                            src="../projects_emg_sensors_03.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        The raw amplified signal is recorded from the Biceps Brachii. Fluctuations in the signal
                        increase when the muscle is activated, with intervals of muscle contraction highlighted by red
                        lines.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            In summary, the crucial aspect is that the variability of the signal encodes muscle activity. The process of
            extracting data from the raw signal involves the following steps:
            <br/>
            <ul>
                <li>
                    Remove the baseline of the signal.
                </li>
                <li>
                    Amplify the signal.
                </li>
                <li>
                    Extract fluctuations using a high-pass filter.
                </li>
                <li>
                    Detect the envelope of the signal.
                </li>
            </ul>
            <br/>
            Fortunately, a circuit module has been designed using the <a target="_blank" rel="noreferrer"
                                                                         href="https://www.analog.com/media/en/technical-documentation/data-sheets/ad8221.pdf">AD8221
            instrumental amplifier</a> that performs these steps
            with analog ICs. The schematic below illustrates the various components of this module, each with its
            designated role.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="circuit"
                            src="../projects_emg_sensors_04.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Block diagram of muscular activity circuit and the role of different parts.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            The next figure displays the input and output of this module.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="circuit input output"
                            src="../projects_emg_sensors_05.png"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Input (blue) and output (red) of the muscular activity module.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            I utilized six modules to record muscular activity from different muscles. The output of each module is
            discretized using 16-bit Analog-to-Digital <a target="_blank" rel="noreferrer"
                                                          href="https://www.ti.com/product/ADS1113">ADS1113
            converters</a> with a
            frequency of approximately 100 Hz. Subsequently, the data is transmitted to the Raspberry Pi (RPi) B4
            running the Ubuntu OS. On the RPi, I executed a Jupyter Notebook server, enabling me to interactively
            visualize real-time measurements.
            <br/><br/>
            I found an intriguing concept. Prior to connecting to the RPi, I ensured the proper functioning of each
            module by measuring the output signal with an impedance meter, powered by batteries. However, upon
            connecting them to the RPi and utilizing their power source for the modules, I consistently encountered
            saturated readings in the output signal. Initially perplexed, I double-checked all the components but could
            not find a solution. Eventually, I speculated that the reason might be the noise carried by the residential
            electricity passing through the RPi adapter. To address this, I decoupled the power source for the sensors
            from both the RPi and the residential electricity, powering them with batteries and connecting everything
            via a decoupler IC, the <a target="_blank" rel="noreferrer"
                                       href="https://www.analog.com/media/en/technical-documentation/data-sheets/adum1250_1251.pdf">ADuM1250</a>.
            This module transmitted digital data (specifically the I2C data) using photons, functioning as an
            opto-coupler. Following this modification, everything worked seamlessly.
            <br/><br/>
            In the final step of sensor calibration, I approached it as a linear model with a slope and a bias term.
            Estimating the bias was straightforward; I short-circuited the input of the sensors to obtain this value. To
            determine the slope term, I connected each electrode to its corresponding muscle and exerted maximum muscle
            contraction. I then assigned this value to the maximum number that the A2D can read. This method ensures the
            utilization of the full range of the A2D.
            <br/><br/>
            <div className="heading-1">Data Filtering on RPi</div>
            <br/><br/>
            After confirming the functionality of all hardware and software components and ensuring correct data
            reading, the next step is to assess data quality. One immediate observation was the presence of
            high-frequency noise on each channel that needed to be addressed. While a straightforward solution might
            involve implementing a digital linear lowpass filter, caution is necessary in designing such a filter as it
            introduces some delay to the data. In a real-time application like this, minimizing delay is crucial—there
            is discomfort if the rehabilitation arm responds 500 milliseconds after the patient's arm movement.
            <br/><br/>
            I explored various designs of linear filters and conducted a comparison. The optimal choice in terms of
            delay turned out to be the <a target="_blank" rel="noreferrer"
                                          href="https://en.wikipedia.org/wiki/Chebyshev_filter">Chebyshev2</a> lowpass
            filter, which
            exhibited a delay of 100 ms. I proposed an idea: since the <a target="_blank" rel="noreferrer"
                                                                          href="https://dsp.stackexchange.com/questions/9467/what-is-the-advantage-of-matlabs-filtfilt">filtfilt</a> method
            provides zero delay
            but is anti-causal, making it unsuitable for real-time applications, I posed the question of identifying the
            most linear filter that closely resembles the filtfilt output. To answer this, I recorded data from the
            sensors, computed the filtfilt of the data, and subsequently trained a single-layer fully-connected network
            using MSE loss. The goal was to find the best filter that closely approximates the filtfilt output.
            Essentially, this process resembled the least square method. Initially, I experimented with adding some
            nonlinearity functions, but they did not yield significant improvements. The result was remarkable: the
            delay was reduced to approximately 50 ms.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="different lowpass filters"
                            src="../projects_emg_sensors_06.png"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Comparison of different linear lowpass filters with my method (Neural Network).
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Task Definition and Recording</div>
            <br/><br/>
            To reach the goal of the project -identify the state of the arm and hand to separate specific conditions- I
            should define some tasks for different subjects and record six muscular activities. Based on different
            combinations of the basic tasks, I defined 30 experiments for subjects to do. I asked them to repeat each
            experiment 5 times with different intensities. These are examples of the experiments:
            <br/>
            <ul>
                <li>
                    Arm extended forward at a 60-degree angle, fingers clenched into a fist, wrist in a neutral
                    position at 0 degrees, elbow with a rotational angle of 90 degrees. Movement: Rotate the elbow from
                    0 degrees to 180 degrees and vice versa.
                </li>
                <li>
                    In a free and neutral position, fingers clenched into a fist with the palm facing inward, wrist in
                    a neutral position at 0 degrees, and elbow with a rotational angle of 90 degrees. Movement: Rotate
                    the elbow outward to a 90-degree angle and then return to a 0-degree angle.
                </li>
                <li>
                    Arm in a free and neutral position at 0 degrees, fingers in an open and extended position, wrist
                    in a neutral position at 0 degrees, and elbow at a 180-degree angle (without pressure). Movement:
                    Rotate the elbow in a circular motion to a -90-degree angle inward and then return to a 0-degree
                    angle.
                </li>
            </ul>
            <br/>
            I interactively monitored data in a Jupyter Notebook to ensure electrode connectivity and signal quality.
            Below, I present examples from two recording sessions.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="example recordings"
                            src="../projects_emg_sensors_07.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Two examples (top and bottom) of muscular activities corresponding to two different experiments.
                        Each experiment involves five repetitions of the same movement.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Classifying arm and hand status with random forest</div>
            <br/><br/>
            As the final step, our aim is to develop a real-time application that can instantly determine the status of
            the hand, encompassing the four conditions mentioned earlier, in addition to the resting state. This problem
            is formulated as a classification problem with five classes, including the resting state. After labeling the
            data using the <a target="_blank" rel="noreferrer"
                              href="https://github.com/Geocene/trainset">Trainset</a>, I organized each segment of
            experiment
            recordings based on its class to create a dataset suitable for classification. Each record in the dataset
            contains a time series with varying length.
            <br/><br/>
            I used the <a target="_blank" rel="noreferrer"
                          href="https://en.wikipedia.org/wiki/Random_forest">Random Forest classifier</a> for two main
            reasons.
            Firstly, it is computationally efficient and can run on the Raspberry Pi with minimal latency, making it
            suitable for real-time applications. Secondly, as an ensemble learning method, it helps prevent overfitting.
            <br/><br/>
            The features of the input for this classifier are constructed at each time-step as a stack of 50 values of
            the filtered signal, sampled at a frequency of 50Hz for each channel. In total, the number of inputs for the
            classifier is 50x6 (50 samples from each of the six channels). I trained the random forest using this
            approach, and the confusion matrix is plotted in the figure below.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="confusion matrix"
                            src="../projects_emg_sensors_08.png"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Confusion matrix of prediction the status of the arm with random forest.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            The image below illustrates an example of real-time arm status prediction. Upon inspection, it appears that
            the classifier successfully predicts the status during transitions from the resting state to another state
            but encounters difficulty in predicting the status when muscle activity remains constant.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="realtime tests"
                            src="../projects_emg_sensors_09.png"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Actual vs predictions of classifier in a real-time application.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Conclusion</div>
            <br/><br/>
            This article explored the process of designing and implementing electrical components and data processing
            approaches for an artificial arm. The importance of low-delay data filtering on the Raspberry Pi was
            emphasized, underscoring the need for minimal delay in real-time applications. This article detailed
            specific experiments to identify arm and hand states, leading to the implementation of a Random Forest
            classifier for real-time status prediction. Despite challenges in predicting constant muscle activity, the
            classifier demonstrated success in transitions between states, showcasing the promising potential of this
            artificial arm.
        </div>
    );
};

const ProjectBodies = [ProjectBody01, ProjectBody02, ProjectBody03];
export default ProjectBodies;
